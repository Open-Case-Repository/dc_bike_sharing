{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://ai.meng.duke.edu'> = <img align=\"left\" style=\"padding-top:10px;\" src='OCR_logo3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Demand for Washington D.C.'s Bike Share System\n",
    "\n",
    "\n",
    "_Version 1.0_  \n",
    "_Author(s): Jon Reifschneider, Duke University School of Engineering_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" style=\"padding-top:10px;\" src=\"CaBi-return2.jpg\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _About this teaching case_\n",
    "**Level:** Beginner  \n",
    "**Language:** Python  \n",
    "**Libraries:** pandas, matplotlib, scikit learn  \n",
    "**Industry:** Transportation & Tourism\n",
    "\n",
    "**Learning Topic(s):**  \n",
    "- Data Manipulation\n",
    "- Exploratory Data Analysis\n",
    "- Feature Engineering & Feature Selection  \n",
    "\n",
    "**Learning Objectives**  \n",
    "- Understand how to create features for time-series data\n",
    "- Learn to perform univariate feature selection to evaluate and reduce features\n",
    "- Learn how to use encoding methods for categorical features\n",
    "- Gain practice in merging, manipulating and visualizing data in pandas and matplotlib\n",
    "\n",
    "**Pre-requisites**  \n",
    "- Basic proficiency in Python and pandas\n",
    "\n",
    "**Case Structure**  \n",
    "This teaching case is structured to follow the ***modified CRISP-DM data science methodology*** used in Duke University's AI for Product Innovation graduate programs. \n",
    "\n",
    "**Datasets Used**  \n",
    "Data used in this case is modified from the following original sources:  \n",
    "Bike sharing data: Capital Bikeshare, available at https://www.capitalbikeshare.com/system-data  \n",
    "Weather data: FreeMeteo weather compiled by University of Porto, available at https://www.kaggle.com/marklvl/bike-sharing-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "[1: Business Understanding](#1)  \n",
    "[2: Data Understanding](#2)  \n",
    "[3: Data Preparation](#3)  \n",
    "[4: Analysis / Modeling](#4)  \n",
    "[5: Evaluation / Interpretation](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Business Understanding <a class=\"anchor\" id=\"1\"></a>\n",
    "\n",
    "You have just been hired as the first data scientist working for Capital Bikeshare, the organization which runs the Washington D.C. bike sharing system. The first major project they have asked you to work on is to build a model to predict demand for the shared bikes in the system for each hour of each day.  \n",
    "\n",
    "Having an accurate understanding of the expected demand is critical to the successful operation of Capital Bikeshare.  If they underestimate demand and have too few bikes available, potential users of the system are not able to find a bike to use and so get upset and are less likely to use the system in the future.\n",
    "\n",
    "In your initial discussions with your new colleagues, you determine that there are two main drivers of demand for bikes:  \n",
    "1) Time  - the demand varies by day and by hour  \n",
    "2) Weather - weather conditions cause fluctuations in demand\n",
    "\n",
    "Our objective in this project is to maximize the accuracy of our prediction model by creating an optimal feature set from the data we have available.  The model you will use has already been set up for you in a separate script (a linear regression model) which you should not change.  Your job is to prepare the data and define an optimal set of features which maximizes the model performance. To evaluate the quality of our model we will use Mean Squared Error (MSE) as our metric.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Data Understanding <a class=\"anchor\" id=\"2\"></a>\n",
    "\n",
    "You have received a csv file of historical demand data from the past two years of operation. The dataset contains the following columns:\n",
    "- dteday : date \n",
    "- hr : hour (0 to 23) \n",
    "- cnt: count of total rental bikes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this before any other code cell\n",
    "# This downloads the csv data files into the same directory where you have saved this notebook\n",
    "\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "import os\n",
    "path = Path()\n",
    "\n",
    "# Dictionary of file names and download links\n",
    "files = {'2011-2012_bikes.csv':'https://storage.googleapis.com/aipi_datasets/2011-2012_bikes.csv',\n",
    "        '2011-2012_weather.csv': 'https://storage.googleapis.com/aipi_datasets/2011-2012_weather.csv'}\n",
    "\n",
    "# Download each file\n",
    "for key,value in files.items():\n",
    "    filename = path/key\n",
    "    url = value\n",
    "    # If the file does not already exist in the directory, download it\n",
    "    if not os.path.exists(filename):\n",
    "        urllib.request.urlretrieve(url,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from model import run_model\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.options.display.float_format = '{:,.2f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-efcc307a64ed278a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "- Read in the file '2011-2012_bikes.csv' and use the name 'data' for your dataframe\n",
    "- Since it is a time series, convert the index to datetime (including day and time)\n",
    "- Convert dteday to a numerical feature which stores the day of year\n",
    "- Then run the next cell to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f133e3af6f71d43e",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run our model using the raw data to get a baseline of modeling performance. We will train our model on the data from January 1, 2011 - June 30, 2012.  We will then use our trained model to predict the bike demand for each day in the period July 1, 2012 - December 31, 2012.\n",
    "\n",
    "The function *run_model* in the code cell below accepts a pandas dataframe containing both the data and the target (must be named 'cnt').  The function will split the data as above, train a model, and calculate the Mean Squared Error of the predictions for the time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_score = run_model(data)\n",
    "print('Mean Squared Error: {:.2f}'.format(mse_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our R-squared is not very high (it ranges from 0 to 1 depending on how much of the variability in the target data the model is able to explain).  We now have a baseline value for R-squared and MSE.  Let's see if we can improve it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add in weather data  \n",
    "Let's now add in weather data from freemeteo.com (compiled by University of Porto).  The new features we will add are:\n",
    "- weathersit : \n",
    "    - 1: Clear, Few clouds, Partly cloudy, Partly cloudy \n",
    "    - 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist \n",
    "    - 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds \n",
    "    - 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \n",
    "- temp : Temperature in Celsius\n",
    "- atemp: Feeling temperature in Celsius\n",
    "- hum: Humidity\n",
    "- windspeed: Wind speed\n",
    "\n",
    "Read in the weather data from '2011-2012_weather.csv'  \n",
    "- Merge the weather data into your existing dataframe *data* (you might have to adjust a column or index in the weather dataframe in order to merge it into your existing dataframe.  Use an 'inner' merge\n",
    "- We have one categorical variable in the weather data: 'weathersit'. Determine if/how you would like to encode it and do so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-551db00107882eb6",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run our model again to get a new baseline on the raw data including the weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_score = run_model(data)\n",
    "print('Mean Squared Error: {:.2f}'.format(mse_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Prepare Data <a class=\"anchor\" id=\"3\"></a>\n",
    "\n",
    "As we have seen above, our new features did not cause much improvement in our r-squared / MSE scores.  This may be because the model treated them as numerical continuous variables, rather than categorical.  For some features that are either 0/1 this will not matter, but for other features that have more than 2 possible values this will make a difference.\n",
    "\n",
    "Let's now encode our categorical features and see if that improves our performance. The choice of which features to encode and which method of encoding (label encoding, ordinal encoding, or one-hot encoding) is up to you.  See the example scripts from class for the code to do the encoding.  Encode any features you wish and store your updated data in a dataframe *data_encoded*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering\n",
    "Let's create some additional features in our data which help the model better understand changing usage patterns over time.  Create some new categorical features which you think will help explain the variance in demand over time.  The features you create should be numerical (do not yet create any categorical/string features).  For now, treat them as numerical continuous features and do not encode them.\n",
    "\n",
    "Example: you might want to create a 'workingday' feature which stores a 0 or 1 depending if the day is a work day (e.g. not a weekend nor a holiday).  \n",
    "\n",
    "Some possible features you might consider: year, month, holiday (whether holiday or not - hint see imports cell for package to use), day of week, working day, etc.  \n",
    "\n",
    "You can create these or create others, it is up to your discretion as the lead data scientist. Reminder: create the features as numerical, but do not yet encode them using any type of encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4a9722be8eedefac",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2bb749f29b4d55b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Use visualizations and/or statistics / statistical tests to determine if the new features you have created are likely to have value in improving our model.  You may choose what analyses to display.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-39d61dff6e952c65",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7539319c363dba8f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Feature Selection\n",
    "\n",
    "We will now analyze our set of features to determine if we have unnecessary features we can remove - features that either add no value to our model or are duplicative with other features.\n",
    "\n",
    "We evaluate features using only the data available to us in the training dataset that is later used to train the model (we do not want to \"peek\" at the test dataset and allow it to influence our choice of variables to use).  First, create a dataset to use for feature selection containing only the data used for model training (the time period Jan 1 2011 - June 30 2012). Remember, if you have one-hot encoded any features, you will need to go back to your data prior to doing the one-hot encoded and use that to run your categorical feature selection - the categorical variables must be label-encoded but not one-hot encoded in order for it to run properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous feature selection\n",
    "Use univariate feature selection methods to evaluate the continuous features in our dataset and determine if we have any unnecessary features to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a150c7f24d26ee26",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a9ee961974408068",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Categorical feature selection\n",
    "\n",
    "Use a univariate method to evaluate the categorical features in your dataset.  R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d9d96ff806bf3097",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-74a1e0e2da72231a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Remove irrelevant or duplicative features\n",
    "\n",
    "Based on the feature selection work you did above, remove any duplicative or unnecessary features which do not add value to your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-660fddb81ed01d9d",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain below why you dropped each of the features you decided to drop (if any):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-bcd7ca5519dcc4b6",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode categorical variables  \n",
    "Let's now encode our categorical features and see if that improves our performance. The choice of which features to encode and which method of encoding (label encoding, ordinal encoding, or one-hot encoding) is up to you.  See the example scripts from class for the code to do the encoding.  Encode any features you wish and store your updated data in a dataframe *data_encoded*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9ba93241d4afe14f",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c15a557fa0b8496a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Standardize continuous features\n",
    "Extract the features ['temp','atemp','hum','windspeed'] from your full dataframe *data_encoded*, standardize each one and store the dataframe containing only the standardized continuous features as *data_standardized*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-8aa6649a6aa39a56",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Modeling <a class=\"anchor\" id=\"4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created, selected and prepared our features, we are ready to run our model again to evaluate our new performance. Run the cell below to re-run the model using your new data (stored as *data_standardized*).  Then, comment on how the performance of your model has changed and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_score = run_model(data_standardized)\n",
    "print('Mean Squared Error: {:.2f}'.format(mse_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Evaluation/Interpretation <a class=\"anchor\" id=\"5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How has the performance of your model changed as you have created new features and used encoding to make them available to your model? Why has this improved the model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
